{"componentChunkName":"component---src-templates-blog-post-js","path":"/aboutLLM/","result":{"data":{"site":{"siteMetadata":{"title":"neekoblog"}},"markdownRemark":{"id":"a29d61e4-f3c5-5c28-807d-0ad703f918d0","excerpt":"LLM이란? LLM은 대화형 인공지능 모델이다. 더 자세히 말하면 LLM은 언어를 이해하고 생성하기 위해 많은 데이터를 소비할 수 있는 딥 러닝 알고리즘이다. LLM은 신경망 아키텍처를 사용하여 언어 모델링을 수행하는 딥 러닝 모델이다. LLM의 특징 LLM은 transformer…","html":"<h3>LLM이란?</h3>\n<p>LLM은 대화형 인공지능 모델이다.</p>\n<p>더 자세히 말하면 LLM은 언어를 이해하고 생성하기 위해 많은 데이터를 소비할 수 있는 딥 러닝 알고리즘이다.</p>\n<p>LLM은 신경망 아키텍처를 사용하여 언어 모델링을 수행하는 딥 러닝 모델이다.</p>\n<h3>LLM의 특징</h3>\n<p>LLM은 transformer modal을 이용한다.\n이것은 LLM의 일반적인 아키텍처이다. 이것은 인코더와 디코더로 구성된다.\n이 모델은 input을 토큰화 하고 그 토큰과의 관계를 수학적 방정식을 동시에 수행해서 데이터를 처리한다.</p>\n<h3>LLM의 구성요소</h3>\n<p>LLM은 여러개의 신경망 층으로 구성되어있다.</p>\n<p><strong>1. Embedding Layer</strong>\n입력 토큰(입력된 텍스트 등)로부터 임베딩 벡터를 생성한다.\nex) \"강아지\" -> [0.2, 0.8, 0.1, 0.5, 0.9]\n여기서 각각 배열의 값은 토큰의 차원을 나타낸다.\n위의 예시를 보면\n첫 번째 숫자: 동물과의 관련성\n두 번째 숫자: 애완동물과의 관련성\n세 번째 숫자: 크기 특성\n네 번째 숫자: 친근함 정도\n다섯 번째 숫자: 포유류와의 관련성\n이런식으로 여러개의 차원으로 해석될 수 있다. 아마 더 많은 차원이 있을 것이다.</p>\n<p><strong>2. Feed Forward Layer(FFN)</strong>\n여기서는 임베딩 된 백터를 처리하여 모델이 더 높은 수준의 추상화를 파악할 수 있게 한다.\n예를 들어서 \"강아지는 귀여운 동물이다.\"\n임베딩 한 결과를 계산하는 계산하는 층이라고 생각하면 된다.\n여기서 긍정적인 감정이 있다는 것과 동물의 개념을 더하는 과정을 통해 복잡한 개념을 이해하도록 한다.</p>\n<p><strong>3. Recurrent Layer</strong>\n해당 층에서는 입력된 텍스트의 단어를 순서대로 해석하며 문장 내 단어들간의 관계를 파악한다.</p>\n<p><strong>4. Attention Mechanism</strong>\n각 텍스트에서 중요한 부분을 강조하는 층인데\n\"강아지는\"에서 \"강아지\"에 집중하고 \"동물이다\"에서 \"동물\"에 집중하는 것이다.\n이런 방식으로 모델이 문장의 의미를 더 정확하게 이해할 수 있게 한다.</p>\n<p><strong>5. Output Layer</strong>\n이 층에서는 모델이 최종적으로 생성한 토큰을 출력한다.</p>\n<h3>LLM의 유형</h3>\n<ol>\n<li>Generic or raw language models(일반 또는 기본 언어 모델)\n학습 데이터의 언어를 기반으로 모델을 학습시킨다.\n정보 검색 및 문서 요약 등에 사용된다.</li>\n<li>Instruction-tuned language models(명령어 튜닝 언어 모델)\n명령어에 대한 응답을 예측하도록 학습시킨다.\n감정 분석을 하거나 텍스트 또는 코드를 생성할 수 있다.</li>\n<li>Dialog-tuned language models(대화 튜닝 언어 모델)\n대화를 나눌 수 있도록 훈련된다.\n심심이 같은 모델이다.</li>\n</ol>\n<h3>LLM이 작동하기까지</h3>\n<p>LLM이 출력 즉, 작동하기 전에 먼저 작업되어야 하는 부분이 존재한다.\nTraining, Fine-tuning, Prompt-tuning</p>\n<p><strong>1. Training</strong>\n위키피디아, GitHub등 사이트에서 데이터를 가지고 와서 사전에 훈련된다.\n여기서는 지침을 주지 않고 주어진 데이터를 처리한다.\n즉, 단어의 의미, 단어의 관계, 문맥에 따른 단어 구분을 학습한다.\nright가 correct를 의미하면서 left의 반대를 의미하는지 등을 분석한다.</p>\n<p><strong>2. Fine-tuning</strong>\n모델을 더 정확하게 학습시키는 과정이다.\n번역 상황인지, 법률 문서 작성의 상황인지에 따라서 계약서를 작성해달라고 했을때 차이가 있는 이유는 이 과정에서 학습이 되기 때문이다.</p>\n<p><strong>3. Prompt-tuning</strong>\n이 단계는 Fine-tuning과 비슷하다.\nFew-shot prompting과 Zero-shot prompting을 통해 작업이 이루어진다.</p>\n<p><code class=\"language-text\">Prompting이란 컴퓨터와 대화하는 방식으로 임무를 지시하는 코딩이다.</code></p>\n<ul>\n<li>\n<p>Zero-shot prompting\n이것은 모델이 풀어야 할 문제(Task)를 입력하고 풀어야 할 대상(Prompt)을 입력하면 모델이 학습하는 방식이다.\nex) \"Hello\"를 번역해줘\n여기서 Hello는 Task이고 번역해줘는 것이 Prompt이다.</p>\n</li>\n<li>\n<p>One-shot prompting: 모델에게 한 개의 예시를 보여주고 그 예시를 기반으로 모델이 학습하는 방식이다.\nex) 한국어를 영어로 번역해줘. 안녕 -> Hello 환영해 ->\nHello가 번역된 것이 하나의 예시이고 한국어를 영어로 번역해달라는 부분이 task이다.</p>\n</li>\n<li>\n<p>Few-shot prompting: 모델에게 몇 개의 예시를 보여주고 그 예시를 기반으로 모델이 학습하는 방식이다.\n위의 예시에서 한가지만 보여줬지만 더 많은 예시를 보여주고 학습시킨다.</p>\n</li>\n<li>\n<p>Chain of thought prompting: 모델에게 생각하는 방법 등 가이드를 제시하는 방법이다.\nex)</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"javascript\"><pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">let</span> z <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nconsole<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">// z에 10을 선언했으니 console.log(z)는 10을 출력한다.</span>\n\n<span class=\"token keyword\">let</span> y <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\nconsole<span class=\"token punctuation\">.</span><span class=\"token function\">log</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n<span class=\"token comment\">//?</span></code></pre></div>\n<p>위의 코드에서 주석에 생각하는 방법을 설명하는 것이 가이드이다.</p>\n<ul>\n<li>Zero-shot Chain of thought: 모델에게 생각하는 방법을 알려주지 않고 모델이 생각하는 방법을 추론하도록 한다.\nex)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"javascript\"><pre class=\"language-javascript\"><code class=\"language-javascript\"><span class=\"token keyword\">let</span> x <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n<span class=\"token keyword\">let</span> y <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\n<span class=\"token keyword\">let</span> z <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> y\n<span class=\"token comment\">// 단계별로 생각해보자</span>\n\n<span class=\"token comment\">//출력</span>\n<span class=\"token comment\">// 1. x에 10을 선언한다.</span>\n<span class=\"token comment\">// 2. y에 20을 선언한다.</span>\n<span class=\"token comment\">// 3. z에 x와 y를 더한 값을 선언한다.</span>\n<span class=\"token comment\">// 4. z를 출력한다.</span></code></pre></div>\n<p>문구 하나만으로 모델이 생각하는 방법을 추론하도록 한다.</p>\n<p><code class=\"language-text\">+ zero shoot 보다 few shot이 더 정확하며, few shot에서 2개 이상의 예시를 들었을 때의 차이는 크지 않다. zero shot을 쓰는 것보다는 cot을 쓰는 것이 더 정확하다.</code></p>\n<p>추천 LLM</p>\n<ol>\n<li>GPT-NeoX-20B</li>\n<li>GPT-J-6b</li>\n<li>Llama 2</li>\n<li>BLOOM</li>\n<li>Falcon</li>\n<li>CodeGen</li>\n<li>BERT</li>\n<li>T5</li>\n<li>GMixtral 8x7B</li>\n</ol>\n<hr>\n<p><a href=\"https://rimiyeyo.tistory.com/entry/%EB%8B%A4%EC%96%91%ED%95%9C-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81Prompt-Engineering%EC%97%90-%EB%8C%80%ED%95%B4-%EC%82%B4%ED%8E%B4%EB%B3%B4%EC%9E%901-Zero-shot-One-shot-Few-shot-CoT#LLM%20Prompt%20Engineering-1\">라미에오 기록저장소 - 출처1</a></p>\n<p><a href=\"https://www.elastic.co/what-is/large-language-models\">Elastic What is LLM - 출처</a></p>\n<p><a href=\"https://www.elastic.co/blog/open-source-llms-guide\">Elastic LLS 추천 2024 - 출처</a></p>","frontmatter":{"title":"LLM에 대해 알아보기","date":"February 09, 2025","description":"LLM에 대해 알아보기"}},"previous":{"fields":{"slug":"/dogaiSerise1/"},"frontmatter":{"title":"강아지 ai 봇 만들기 1탄"}},"next":{"fields":{"slug":"/requestAnimationFrame/"},"frontmatter":{"title":"RequestAnimationFrame"}}},"pageContext":{"id":"a29d61e4-f3c5-5c28-807d-0ad703f918d0","previousPostId":"687e4e45-a60f-5dfc-8465-6a7733d33b37","nextPostId":"78bf7f03-e38f-57e7-a961-ce1859a2396e"}},"staticQueryHashes":["2841359383","3814644374"],"slicesMap":{}}